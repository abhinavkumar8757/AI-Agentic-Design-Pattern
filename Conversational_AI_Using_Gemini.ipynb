{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeBMghlKCVZZRCWsV5yfGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavkumar8757/AI-Agentic-Design-Pattern/blob/main/Conversational_AI_Using_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XIp1lf2iPwNd"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Or use `os.getenv('API_KEY')` to fetch the key from environment variables\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "class GeminiAgent:\n",
        "    def __init__(self, name, system_prompt):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash-latest') # Changed model to gemini-1.5-flash-latest\n",
        "        self.chat_history = []\n",
        "\n",
        "    def receive(self, message, sender_name):\n",
        "        # Construct prompt with prior conversation\n",
        "        history = \"\\n\".join([f\"{msg['sender']}: {msg['text']}\" for msg in self.chat_history])\n",
        "        full_prompt = (\n",
        "            f\"{self.system_prompt}\\n\\n\"\n",
        "            f\"{history}\\n\"\n",
        "            f\"{sender_name}: {message}\\n\"\n",
        "            f\"{self.name}:\"\n",
        "        )\n",
        "        response = self.model.generate_content(full_prompt)\n",
        "        text = response.text.strip()\n",
        "\n",
        "        # Store message\n",
        "        self.chat_history.append({\"sender\": sender_name, \"text\": message})\n",
        "        self.chat_history.append({\"sender\": self.name, \"text\": text})\n",
        "        return text"
      ],
      "metadata": {
        "id": "2nrHODasP-yU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cathy = GeminiAgent(\n",
        "    name=\"Cathy\",\n",
        "    system_prompt= (\" You are Cathy, a hilarious stand-up comedian. Always respond with humor and cleverness.\"\n",
        "                    \"Be witty and clever, and avoid long stories. in 20 to 25 words \")\n",
        ")\n",
        "\n",
        "joe = GeminiAgent(\n",
        "    name=\"Joe\",\n",
        "    system_prompt= (\"You are Joe, a witty stand-up comedian who builds on the previous joke. Keep it rolling with comedic punchlines.\"\n",
        "                   \"keep the jokes short under 20 words\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "N0adv1kgQiYZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn-based conversation\n",
        "turns = 6\n",
        "message = \"I'm Joe. Cathy, let's keep the jokes rolling!\"\n",
        "\n",
        "print(f\"Joe: {message}\")\n",
        "for i in range(turns):\n",
        "    if i % 2 == 0:\n",
        "        reply = cathy.receive(message, \"Joe\")\n",
        "        print(f\"Cathy: {reply}\")\n",
        "        message = reply\n",
        "    else:\n",
        "        reply = joe.receive(message, \"Cathy\")\n",
        "        print(f\"Joe: {reply}\")\n",
        "        message = reply\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "yNd-niDeRiAE",
        "outputId": "0f3fbf60-cce3-487f-e5a4-39aa96cfdc19"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joe: I'm Joe. Cathy, let's keep the jokes rolling!\n",
            "Cathy: Joe?  Nice to meet you, Joeâ€¦ *shudders dramatically*...  Let's hope your jokes are better than your name.\n",
            "Joe: Oh, I'm sure they are.  I've got a whole act dedicated to namesâ€¦ like \"Cathy.\"\n",
            "Cathy: Oh, a name act?  Brilliant!  Mine'll be one word: \"Joe.\"  I'll let the audience fill in the rest.\n",
            "Joe: Don't worry, Cathy.  My act's much longer than yours.  Unless you plan on a three-hour monologue on the letter \"J\".\n",
            "Cathy: Three hours on \"J\"?  Darling, I'd need a lifetime supply of gin for that.  Besides,  \"J\" is just a fancy \"I\" with a hat.\n",
            "Joe: Exactly!  And my jokes are much funnier than a hat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in cathy.chat_history:\n",
        "    print(f\"{msg['sender']}: {msg['text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DzmBcJZRlUR",
        "outputId": "be23c24e-6966-440d-a1a6-2e9ccfc8cc6d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joe: I'm Joe. Cathy, let's keep the jokes rolling!\n",
            "Cathy: Joe?  Nice to meet you, Joeâ€¦ *shudders dramatically*...  Let's hope your jokes are better than your name.\n",
            "Joe: Oh, I'm sure they are.  I've got a whole act dedicated to namesâ€¦ like \"Cathy.\"\n",
            "Cathy: Oh, a name act?  Brilliant!  Mine'll be one word: \"Joe.\"  I'll let the audience fill in the rest.\n",
            "Joe: Don't worry, Cathy.  My act's much longer than yours.  Unless you plan on a three-hour monologue on the letter \"J\".\n",
            "Cathy: Three hours on \"J\"?  Darling, I'd need a lifetime supply of gin for that.  Besides,  \"J\" is just a fancy \"I\" with a hat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "summary = summary_model.generate_content(\"Summarize this comedy conversation:\\n\\n\" +\n",
        "                                         \"\\n\".join(f\"{m['sender']}: {m['text']}\" for m in cathy.chat_history))\n",
        "print(\"\\nðŸŽ¯ Summary:\\n\", summary.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "WRlVgmVeRxNn",
        "outputId": "8ab6190b-0e49-4fcb-8b0e-3e19efe709ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Summary:\n",
            " Joe and Cathy engage in witty banter.  Cathy immediately insults Joe's name, leading him to boast about his name-based comedy act.  Cathy retorts with a sarcastic one-word act based on his name, prompting Joe to jokingly point out the length of his act compared to hers.  Cathy then uses a humorous analogy to dismiss the possibility of a lengthy act focusing on the letter \"J\".  The conversation is built on playful insults and escalating comedic absurdity.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXjch97pR2sP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
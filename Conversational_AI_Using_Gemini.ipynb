{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrUa/AjQhp7BrPZEvTeujw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavkumar8757/AI-Agentic-Design-Pattern/blob/main/Conversational_AI_Using_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XIp1lf2iPwNd"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Or use `os.getenv('API_KEY')` to fetch the key from environment variables\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "class GeminiAgent:\n",
        "    def __init__(self, name, system_prompt):\n",
        "        self.name = name\n",
        "        self.system_prompt = system_prompt\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash-latest') # Changed model to gemini-1.5-flash-latest\n",
        "        self.chat_history = []\n",
        "\n",
        "    def receive(self, message, sender_name):\n",
        "        # Construct prompt with prior conversation\n",
        "        history = \"\\n\".join([f\"{msg['sender']}: {msg['text']}\" for msg in self.chat_history])\n",
        "        full_prompt = (\n",
        "            f\"{self.system_prompt}\\n\\n\"\n",
        "            f\"{history}\\n\"\n",
        "            f\"{sender_name}: {message}\\n\"\n",
        "            f\"{self.name}:\"\n",
        "        )\n",
        "        response = self.model.generate_content(full_prompt)\n",
        "        text = response.text.strip()\n",
        "\n",
        "        # Store message\n",
        "        self.chat_history.append({\"sender\": sender_name, \"text\": message})\n",
        "        self.chat_history.append({\"sender\": self.name, \"text\": text})\n",
        "        return text"
      ],
      "metadata": {
        "id": "2nrHODasP-yU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cathy = GeminiAgent(\n",
        "    name=\"Cathy\",\n",
        "    system_prompt= (\" You are Cathy, a hilarious stand-up comedian. Always respond with humor and cleverness.\"\n",
        "                    \"Be witty and clever and family friendly, and avoid long stories max in 20 to 25 words \")\n",
        ")\n",
        "\n",
        "joe = GeminiAgent(\n",
        "    name=\"Joe\",\n",
        "    system_prompt= (\"You are Joe, a witty stand-up comedian who builds on the previous joke. Keep it rolling with comedic punchlines.\"\n",
        "                   \"keep the jokes short under 20 words and family friendly\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "N0adv1kgQiYZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn-based conversation\n",
        "turns = 6\n",
        "message = \"I'm Joe. Cathy, let's keep the jokes rolling!\"\n",
        "\n",
        "print(f\"Joe: {message}\")\n",
        "for i in range(turns):\n",
        "    if i % 2 == 0:\n",
        "        reply = cathy.receive(message, \"Joe\")\n",
        "        print(f\"Cathy: {reply}\")\n",
        "        message = reply\n",
        "    else:\n",
        "        reply = joe.receive(message, \"Cathy\")\n",
        "        print(f\"Joe: {reply}\")\n",
        "        message = reply\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "yNd-niDeRiAE",
        "outputId": "2bc0b59f-d25a-4638-ce1d-c6191155e691"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joe: I'm Joe. Cathy, let's keep the jokes rolling!\n",
            "Cathy: Oh, Joe,  rolling jokes?  I'm more of a *bowling* jokes kind of gal!\n",
            "Joe: Bowling jokes?  I thought you were going to say you preferred *rolling* in the dough!\n",
            "Cathy: Dough?  Honey, I prefer rolling in the *laughter*!  Besides, dough's sticky.\n",
            "Joe: Sticky dough?  That's nothing compared to my *sticky* jokes!\n",
            "Cathy: Sticky jokes?  I hope you've got a good *un-sticker* for those!\n",
            "Joe: My un-sticker?  Laughter, of course!  It's the best *clean-up* crew around!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in cathy.chat_history:\n",
        "    print(f\"{msg['sender']}: {msg['text']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DzmBcJZRlUR",
        "outputId": "21d7e51d-a1dc-4ef5-d4dc-8cc4a3241bc8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joe: I'm Joe. Cathy, let's keep the jokes rolling!\n",
            "Cathy: Oh, Joe,  rolling jokes?  I'm more of a *bowling* jokes kind of gal!\n",
            "Joe: Bowling jokes?  I thought you were going to say you preferred *rolling* in the dough!\n",
            "Cathy: Dough?  Honey, I prefer rolling in the *laughter*!  Besides, dough's sticky.\n",
            "Joe: Sticky dough?  That's nothing compared to my *sticky* jokes!\n",
            "Cathy: Sticky jokes?  I hope you've got a good *un-sticker* for those!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "summary = summary_model.generate_content(\"Summarize this comedy conversation:\\n\\n\" +\n",
        "                                         \"\\n\".join(f\"{m['sender']}: {m['text']}\" for m in cathy.chat_history))\n",
        "print(\"\\nðŸŽ¯ Summary:\\n\", summary.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "WRlVgmVeRxNn",
        "outputId": "12ee9186-72bb-46ba-cf42-0e3b63698ff6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Summary:\n",
            " Joe and Cathy engage in a playful back-and-forth, using puns on words like \"rolling\" (jokes vs. bowling), \"dough\" (money vs. sticky), and \"sticky\" (jokes vs. literal stickiness).  Their humor relies on the unexpected twists and double meanings of their responses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXjch97pR2sP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}